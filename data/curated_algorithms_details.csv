Who initially entered the data,Curator choses that,,,,,,,,,,,HH:MM:SS,int,.,,"float, in GB",Baker Cluster / CSD3 / cloud computing / local laptop or desktop,,"Needs to match the list ""TDP_cpu"" or ""TDP_gpu""","otherwise, average value",,in h ,in W,in kW,in kWh,,in kgCO2e,,,,,Total impact in kgCO2e,,,,,,,,,,,,,,,
Name,ID,Task,Name of the tool,Selected for publication,Benchmark or not,Group_1,Group_2,Group_3,Group_4,Extra details about what you're doing (incl. source),Pragmatic scaling factor,Running time,Number of cores,CPU or GPU,Mem requested or used,Memory,computing resources,PUE value used,CPU model,PUE if known,Source PUE,Number of hours,Core power draw,Power draw,Energy consumption,Different location if needed,World average impact,tree years,tree-months,km in a car (US),km in a car (EU),ZA,IN,AU-VIC,CN,JP,US,IE,DE,RU,IT,GB,US-MA,CA,FR,NZ,CH
Local impact in kgCO2e/kWh,,,,,,,,,,,,.,.,.,,.,,,.,,,,,,,,0.5,,,,,0.961,0.743,1.170,0.624,0.492,0.476,0.393,0.469,0.330,0.327,0.277,0.393,0.140,0.047,0.105,0.014
Loïc,chloroplast_assembly_GetOrganelle_1,Chloroplast genome assembly,GetOrganelle ,,Benchmark,,,,,"From https://www.biorxiv.org/content/10.1101/665869v1 / Running time had t be estimated from a graph, so not very accurate / input size 2.5M",,0:50:00,1,CPU,,5.25,local server,1.58,Xeon E7-8867 v3,,5.01,0.83,10.3,0.012,0.017,,0.008,0.001,0.009,0.032,0.046,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,chloroplast_assembly_GetOrganelle_8,Chloroplast genome assembly,GetOrganelle ,,Benchmark,,,,,idem,,0:26:40,8,CPU,,6.07,local server,1.58,Xeon E7-8867 v3,,,0.44,10.3,0.085,0.063,,0.030,0.003,0.031,0.119,0.171,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,chloroplast_assembly_FastPlast_1,Chloroplast genome assembly,Fast-Plast,,Benchmark,,,,,idem,,2:46:40,1,CPU,,6.8,local server,1.58,Xeon E7-8867 v3,,,2.78,10.3,0.013,0.060,,0.028,0.002,0.030,0.113,0.162,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,chloroplast_assembly_FastPlast_8,Chloroplast genome assembly,Fast-Plast,,Benchmark,,,,,idem,,0:50:00,8,CPU,,6.87,local server,1.58,Xeon E7-8867 v3,,,0.83,10.3,0.085,0.118,,0.056,0.005,0.059,0.224,0.321,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jonathan,tensorQTL_1,Cis-eQTL calling,TensorQTL,,,,,,,"2,745 samples, 17,674 phenotypes, 41 covariates, 10,713,729 variants. cis_nominal flag, default parameters. Run on GPU, 2 nodes",,1:14:26,8,GPU,,192,CSD3,1.15,Tesla P100 PCIe,,,1.24,250,2.072,4.292,,2.039,0.179,2.146,8.122,11.649,4.1,3.2,5.0,2.7,2.1,2.0,1.7,2.0,1.4,1.4,1.2,1.7,0.6,0.2,0.5,0.1
Guillaume,metagenomeClassif_centrifuge_1,Metagenome classification,Centrifuge,,,,,,,"Classification of a human metagenomic sample (SRA accession SRS144560, sampled from the tongue dorsum of a male participant in the dbGaP study ""HMP Core Microbiome Sampling Protocol A (HMP-A); number of paired reads: 158,172,898) using Centrifuge (version 1.0.41) and default GTDB database (release 89, 2019)2,3.",,8:20:39,1,CPU,Requested,256,Baker cluster,1.58,Xeon E5-2683 v4,,,8.34,7.5,0.103,1.433,,0.681,0.060,0.717,2.712,3.890,1.4,1.1,1.7,0.9,0.7,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.2,0.0
,,Exact usage,,,,,,,,,,8:20:39,1,CPU,Used,116.7,Baker cluster,1.58,Xeon E5-2683 v4,,,8.34,7.5,0.051,0.710,,0.337,0.030,0.355,1.344,1.928,0.7,0.5,0.8,0.4,0.3,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.1,0.0,0.1,0.0
Loïc,,Metagenome classification,Centrifuge,,Benchmark,,,,,"The genomes used were Homo sapiens hg19. Human data were limited to chromosomes 1–22, X and Y. For human data, 30,000 transcript models were chosen at random from a conglomeration of 858,063 gene models obtained by taking the union of ten annotation tracks: RefSeq, GeneID, Aceview, Augustus, ENSEMBL, UCSC, Vega, GenCode, GenScan, and lincRNA.- Simulated data from humans containing 10 million 100 basre read pairs (2x10^9 bases) The most complex forms of the simulate dataset were used here (T3) which had high polymorphism and error rates (0.03 substitution, 0.005 indel, and 0.02 error) - online methods. A job was designed for each alignment run and 16 threads were reserved for each job. (https://www.med.upenn.edu/hpc/) -Intel E5-2665 2.4Ghz Xeon Processors (115 W) - note memory is exact not requested",,0:58:00,1,CPU,,12,,,Any,,,0.97,12.0,0.016,0.027,,0.013,0.001,0.013,0.050,0.072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,,Metagenome classification,Kraken2,,Benchmark,,,,,,,0:20:00,1,CPU,,21,,,Any,,,0.33,12.0,0.020,0.011,,0.005,0.000,0.006,0.021,0.030,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,,Metagenome classification,Kraken/Braken,,Benchmark,,,,,,,1:40:00,1,CPU,,154,,,Any,,,1.67,12.0,0.069,0.193,,0.092,0.008,0.097,0.365,0.524,0.2,0.1,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0
Guillaume,bacterialGenomeAssembly_unicycler_1,Bacterial genome assembly,Unicycler,,,,,,,Assembly of raw sequencing reads from an isolate of Klebsiella pneumoniae from pyogenic liver abscess infections in Singapore (SRA accession: SRR6307304) using the SPAdes-based assembler Unicycler4.,,0:29:57,1,CPU,Requested,16,Baker cluster,1.58,Xeon E5-2683 v4,,,0.50,7.5,0.013,0.011,,0.005,0.000,0.006,0.021,0.030,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
,,Exact usage,,,,,,,,,,0:29:57,1,CPU,Used,4.73,Baker cluster,1.58,Xeon E5-2683 v4,,,0.50,7.5,0.009,0.008,,0.004,0.000,0.004,0.015,0.021,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Guillaume,bacterialPangenome_roary_1,Bacterial pangenome discovery pipeline,Roary,,,,,,,Bacterial pangenome discovery pipeline on 176 complete Mycobacterium tuberculosis genomes publicly available on NCBI using Roary (https://sanger-pathogens.github.io/Roary/).,,2:29:06,16,CPU,Requested,64,MASSIVE,1.58,Xeon E5-2680 v3,,,2.49,10.0,0.184,0.763,,0.362,0.032,0.381,1.444,2.071,0.7,0.6,0.9,0.5,0.4,0.4,0.3,0.4,0.3,0.2,0.2,0.3,0.1,0.0,0.1,0.0
,,Exact usage,,,,,,,,,,2:29:06,16,CPU,Used,8.03,MASSIVE,1.58,Xeon E5-2680 v3,,,2.49,10.0,0.163,0.676,,0.321,0.028,0.338,1.280,1.836,0.6,0.5,0.8,0.4,0.3,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.1,0.0,0.1,0.0
Woei Yuh,RNAseq_fastQC_1,CAS-RNAseq: QC check on rawData,FastQC,,,,,,,"First round of QC check on the raw reads of 538 samples + 51 repeated samples. However, QC of 197 samples has already completed in Dec 2019. So, current estimate is based on 392 samples. ",,370:10:13,18,CPU,,8,Baker cluster,1.58,Xeon E5-2683 v4,,,370.17,7.5,0.138,85.297,,40.516,3.554,42.649,161.419,231.521,81.9,63.4,99.8,53.2,41.9,40.6,33.5,40.0,28.2,27.9,23.7,33.5,11.9,4.0,9.0,1.2
Woei Yuh,,CAS-RNAseq: adapter removal,TrimGalore,,,,,,,"Remove adapters from raw reads. Adapter removal of 197 samples has already completed in Dec 2019. So, current estimate is based on 392 samples. ",,79:44:24,18,CPU,,8,,,Xeon E5-2683 v4,,,79.74,7.5,0.138,18.374,,8.728,0.766,9.187,34.772,49.873,17.7,13.7,21.5,11.5,9.0,8.7,7.2,8.6,6.1,6.0,5.1,7.2,2.6,0.9,1.9,0.3
Woei Yuh,,CAS-RNAseq: optical duplicates removal,bbmap/clumpify,,,,,,,"Remove optical duplicate reads from raw reads. Removal of optical duplicate reads of 197 samples has already completed in Dec 2019. So, current estimate is based on 392 samples. ",,11:18:37,16,CPU,,8,,,Xeon E5-2683 v4,,,11.31,7.5,0.123,2.323,,1.103,0.097,1.161,4.396,6.305,2.2,1.7,2.7,1.4,1.1,1.1,0.9,1.1,0.8,0.8,0.6,0.9,0.3,0.1,0.2,0.0
Woei Yuh,,QC step,,,,,,,,,,461:13:14,.,.,,.,,,.,,,,,#VALUE!,#VALUE!,,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!
Woei Yuh,,CAS-RNAseq: Alignment,STARv2.7.0e,,,,,,,Alignment of 392 samples to a human genome (Ensemble Hg38),,23:58:45,32,CPU,,8,,,Xeon E5-2683 v4,,,23.98,7.5,0.243,9.730,,4.622,0.405,4.865,18.414,26.411,9.3,7.2,11.4,6.1,4.8,4.6,3.8,4.6,3.2,3.2,2.7,3.8,1.4,0.5,1.0,0.1
Woei Yuh,,RNAseq: all together,,,,,,,,,,485:11:59,.,.,,.,,,.,,,,,#VALUE!,#VALUE!,,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!
Guillaume,metagenomeAssembly_metaSPAdes_1,Metagenome assembly,metaSPAdes,,,,,,,"Metagenome assembly of 1 human metagenome (21,412,208 raw reads) using metaSPAdes",,4:14:32,32,CPU,,512,Baker cluster,1.58,Xeon E5-2683 v4,,,4.24,7.5,0.431,3.051,,1.449,0.127,1.526,5.775,8.282,2.9,2.3,3.6,1.9,1.5,1.5,1.2,1.4,1.0,1.0,0.8,1.2,0.4,0.1,0.3,0.0
Guillaume,metagenomicMapping_bowtie2_1,Metagenomic read mapping,bowtie2,,,,,,,Mapping reads from 30 metagenomic samples to a human reference (hg19),,2:26:30,32,CPU,,32,Baker cluster,1.58,Xeon E5-2683 v4,,,2.44,7.5,0.252,1.027,,0.488,0.043,0.514,1.944,2.788,1.0,0.8,1.2,0.6,0.5,0.5,0.4,0.5,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Guillaume,metagenomicTrimming_atropos_1,Metagenomic read trimming,atropos,,,,,,,Trimming metagenomic reads for 1 large sample from the HMP (SRA accession: SRS016585) as a test,,6:04:12,1,CPU,,32,Baker cluster,1.58,Xeon E5-2683 v4,,,6.07,7.5,0.019,0.197,,0.094,0.008,0.098,0.373,0.534,0.2,0.1,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0
Fachrul,metagenomicTrimming_trimgalore_1,Metagenomic read trimming,TrimGalore,,,,,,,"Remove adapter and trim paired-end metagenomic reaads from 29 samples (-q 20, --length 20).",,5:31:39,10,CPU,,100,Baker cluster,1.58,Xeon E5-2683 v4,,,5.53,7.5,0.112,1.036,,0.492,0.043,0.518,1.961,2.812,1.0,0.8,1.2,0.6,0.5,0.5,0.4,0.5,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Yang,diseasePred_1,disease prediction,machine learning,,,,,,,clean/process/analyze/train data,,4:00:00,5,CPU,,25,Baker cluster,1.58,Xeon E5-2683 v4,,,4.00,7.5,0.047,0.313,,0.149,0.013,0.156,0.592,0.849,0.3,0.2,0.4,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0
Yang,diseasePred_2,disease prediction,machine learning,,,,,,,clean/process/analyze/train data,,48:00:00,10,CPU,,100,Baker cluster,1.58,Xeon E5-2683 v4,,,48.00,7.5,0.112,8.998,,4.274,0.375,4.499,17.028,24.423,8.6,6.7,10.5,5.6,4.4,4.3,3.5,4.2,3.0,2.9,2.5,3.5,1.3,0.4,0.9,0.1
Scott,PRS_plink2_1,PRS level calculation,Plink2,,,,,,,"Take a file of variants + weights and score UK Biobank participants. 
Run as an array job, 1 task per chromosome -> on CSD3",,1:00:00,22,CPU,,528,CSD3,1.15,Xeon Gold 6142,,,1.00,9.4,0.403,0.673,,0.320,0.028,0.336,1.273,1.826,0.6,0.5,0.8,0.4,0.3,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.1,0.0,0.1,0.0
Scott,PRS_plink2_2,PRS level calculation,Plink2,,,,,,,Take a file of variants + weights and score UK Biobank participants. -> on Baker,,24:00:00,22,CPU,,128,CSD3,1.15,Xeon Gold 6142,,,24.00,9.4,0.254,10.178,,4.834,0.424,5.089,19.260,27.625,9.8,7.6,11.9,6.3,5.0,4.8,4.0,4.8,3.4,3.3,2.8,4.0,1.4,0.5,1.1,0.1
Loïc,google_meena_1,Chatbot,Google's Meena,,,,,,,,,720:00:00,2048,GPU,,341,Google,1.1,TPU3,,,720.00,200,409.727,492655.772,,"234,011.492","20,527.324","246,327.886","932,316.700","1,337,208.524","473,245.1","365,994.0","576,407.3","307,220.1","242,189.6","234,454.9","193,613.7","231,055.6","162,674.9","161,098.4","136,613.4","193,613.7","68,971.8","23,154.8","51,778.1","6,897.2"
Loïc,google_meena_1,Chatbot,Google's Meena,,,,,,,"Chatbot trained from end to end, 30 days on a TPU v3 Pod. We are ignoring memory consumption because TPU draw is quite unclear  from https://arxiv.org/pdf/2001.09977.pdf",,720:00:00,1,GPU,,0,,,TPU v3 pod,,,720.00,288000,288.000,346291.200,,"164,488.320","14,428.800","173,145.600","655,331.952","939,933.257","332,647.3","257,259.7","405,160.7","215,947.2","170,236.8","164,800.0","136,092.4","162,410.6","114,345.4","113,237.2","96,026.5","136,092.4","48,480.8","16,275.7","36,395.2","4,848.1"
Loïc,boltLMM_v23_150k,Mixed models association,BOLT-LMM v2.3,,Benchmark,,,,,"""We analyzed the years-of-education phenotype as a representative trait, and we ran all methods on the same set of all European-ancestry individuals in the UK Biobank, N=150K, analysing ~72M imputed SNPs and imposing a MAF>0.1% filter on minor allele frequency."" from  https://www.nature.com/articles/s41588-018-0144-6",,14:52:46,8,CPU,,0,local server,1.58,Xeon E5-2683 v4,,,14.88,7.5,0.060,1.491,,0.708,0.062,0.745,2.821,4.047,1.4,1.1,1.7,0.9,0.7,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.2,0.0
Loïc,boltLMM_v23_500k,GWAS,BOLT-LMM v2.3,,Benchmark,,,,,"""We analyzed the years-of-education phenotype as a representative trait, and we ran all methods on the same set of all European-ancestry individuals in the UK Biobank, N=500K, analysing ~93M imputed SNPs and imposing a MAF>0.1% filter on minor allele frequency."" from  https://www.nature.com/articles/s41588-018-0144-6",,60:57:36,8,CPU,,0,local server,1.58,Xeon E5-2683 v4,,,60.96,7.5,0.060,6.108,,2.901,0.255,3.054,11.559,16.579,5.9,4.5,7.1,3.8,3.0,2.9,2.4,2.9,2.0,2.0,1.7,2.4,0.9,0.3,0.6,0.1
Loïc,,GWAS,"same, 100 traits",,,,,,,,,.,.,.,,.,,,.,,,,,6.000,610.819,,290.139,25.451,305.410,"1,155.933","1,657.938",586.8,453.8,714.7,380.9,300.3,290.7,240.1,286.5,201.7,199.7,169.4,240.1,85.5,28.7,64.2,8.6
Loïc,boltLMM_v1_150k,GWAS,BOLT-LMM v1,,Benchmark,,,,,"""We analyzed the years-of-education phenotype as a representative trait, and we ran all methods on the same set of all European-ancestry individuals in the UK Biobank, N=150K, analysing ~72M imputed SNPs and imposing a MAF>0.1% filter on minor allele frequency."" from  https://www.nature.com/articles/s41588-018-0144-6",,58:19:12,8,CPU,,0,local server,1.58,Xeon E5-2683 v4,,,58.32,7.5,0.060,5.844,,2.776,0.243,2.922,11.059,15.861,5.6,4.3,6.8,3.6,2.9,2.8,2.3,2.7,1.9,1.9,1.6,2.3,0.8,0.3,0.6,0.1
Loïc,boltLMM_v1_500k,GWAS,BOLT-LMM v1,,Benchmark,,,,,"""We analyzed the years-of-education phenotype as a representative trait, and we ran all methods on the same set of all European-ancestry individuals in the UK Biobank, N=500K, analysing ~93M imputed SNPs and imposing a MAF>0.1% filter on minor allele frequency."" from  https://www.nature.com/articles/s41588-018-0144-6",,224:09:36,8,CPU,,0,local server,1.58,Xeon E5-2683 v4,,,224.16,7.5,0.060,22.461,,10.669,0.936,11.230,42.506,60.965,21.6,16.7,26.3,14.0,11.0,10.7,8.8,10.5,7.4,7.3,6.2,8.8,3.1,1.1,2.4,0.3
Loïc,,GWAS,"same, 100 traits",,,,,,,,,,,.,,.,,,.,,,,,6.000,2246.083,,"1,066.890",93.587,"1,123.042","4,250.556","6,096.512","2,157.6","1,668.6","2,627.9","1,400.7","1,104.2","1,068.9",882.7,"1,053.4",741.7,734.5,622.8,882.7,314.5,105.6,236.1,31.4
Loïc,,Genome classification,Centrifuge,,Benchmark,,,,,"""In an international effort to research the disease and stop its spread, several groups sequenced the Ebola virus collected from patients’ blood samples and released their data sets online
(Baize et al. 2014; Gire et al. 2014; Park et al. 2015). The genomic data were used to trace the disease and mutations in the Ebola genome and inform further public health and research efforts. Lauck et al. (2015) reanalyzed one of the data sets (Gire et al. 2014) in order to assess the prevalence and effect of GB virus C co-infection on the outcome of EVD. We analyzed 130 paired-end sequencing runs from 49 patients reported in Gire et al. (2014) using Centrifuge to look for further co-infections. This data set has a total of 97,097,119 reads (26 GB of FASTA files).For this analysis, we used the database p + h + v containing all prokaryotic genomes (compressed), all viral genomes, and the human genome (total index size: 6.9 GB). Running on a desktop computer (quad-core, Intel Core i5-4460 @ 3.2 GHz with 8 GB RAM), Centrifuge completed the analysis of all samples in 47 min with four cores."" from https://genome.cshlp.org/content/26/12/1721",,0:47:00,4,CPU,,8,desktop computer,1,Core i5-4460,,,0.78,21.0,0.087,0.114,,0.054,0.005,0.057,0.215,0.309,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jason,tophat2_human,Alignment,Tophat2,,Benchmark,,,,,"The genomes used were Homo sapiens hg19. Human data were limited to chromosomes 1–22, X and Y. For human data, 30,000 transcript models were chosen at random from a conglomeration of 858,063 gene models obtained by taking the union of ten annotation tracks: RefSeq, GeneID, Aceview, Augustus, ENSEMBL, UCSC, Vega, GenCode, GenScan, and lincRNA.- Simulated data from humans containing 10 million 100 basre read pairs (2x10^9 bases) The most complex forms of the simulate dataset were used here (T3) which had high polymorphism and error rates (0.03 substitution, 0.005 indel, and 0.02 error) - online methods. A job was designed for each alignment run and 16 threads were reserved for each job. (https://www.med.upenn.edu/hpc/) -Intel E5-2665 2.4Ghz Xeon Processors (115 W) - note memory is exact not requested - from doi:10.1038/Nmeth.4106",,3:37:53,16,CPU,Used,24,UPENN HPC,1.58,Xeon E5-2665,,,3.63,14.4,0.239,1.449,,0.688,0.060,0.725,2.742,3.933,1.4,1.1,1.7,0.9,0.7,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.2,0.0
Jason,star_human,Reads alignments,star,,Benchmark,,,,,"The genomes used were Homo sapiens hg19. Human data were limited to chromosomes 1–22, X and Y. For human data, 30,000 transcript models were chosen at random from a conglomeration of 858,063 gene models obtained by taking the union of ten annotation tracks: RefSeq, GeneID, Aceview, Augustus, ENSEMBL, UCSC, Vega, GenCode, GenScan, and lincRNA.- Simulated data from humans containing 10 million 100 basre read pairs (2x10^9 bases) The most complex forms of the simulate dataset were used here (T3) which had high polymorphism and error rates (0.03 substitution, 0.005 indel, and 0.02 error) - online methods. A job was designed for each alignment run and 16 threads were reserved for each job. (https://www.med.upenn.edu/hpc/) -Intel E5-2665 2.4Ghz Xeon Processors (115 W) - note memory is exact not requested, from Simulation-based comprehensive benchmarking of RNA-seq aligners. Note i scaled time by 30 to get the number of reads for human base pairs. ",,5:21:30,16,CPU,Used,36,UPENN HPC,1.58,Xeon E5-2665,,,5.3583,14.375,0.243,2.178,,1.035,0.091,1.089,4.122,5.912,2.1,1.6,2.5,1.4,1.1,1.0,0.9,1.0,0.7,0.7,0.6,0.9,0.3,0.1,0.2,0.0
Loïc,,Reads alignments,star,,Benchmark,,,,,"Reads alignment on simulated data for human hg19 genome (1 set of 10 million 100-base read pairs - 2x10^9 bases) from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5792058/). Used ""Total run time""",,0:10:37,16,CPU,,35,,,Xeon E5-2665,,,0.1769,14.375,0.243,0.072,,0.034,0.003,0.036,0.136,0.195,0.1,0.1,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,,Reads alignments,star,,Benchmark,,,,,"same, with real CPU usage time",,0:06:40,16,CPU,,35,,,Xeon E5-2665,,,0.1111,14.375,0.243,0.045,,0.021,0.002,0.023,0.085,0.122,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Loïc,,Reads alignments,star,,Benchmark,,,,,x 30,,5:18:30,16,CPU,,35,,,Xeon E5-2665,,,5.3083,14.375,0.243,2.155,,1.023,0.090,1.077,4.077,5.848,2.1,1.6,2.5,1.3,1.1,1.0,0.8,1.0,0.7,0.7,0.6,0.8,0.3,0.1,0.2,0.0
Jason,tophat2_malaria,Alignment,Tophat2,,Benchmark,,,,,"The P. falciparum genome was used because it is notorious for being difficult, mainly because it is approximately 80% AT rich in exons and 90% in introns and intergenic regions - Simulated data from malaria containing 10 million 100 basre read pairs (2x10^9 bases) The most complex forms of the simulate dataset were used here (T3) which had high polymorphism and error rates (0.03 substitution, 0.005 indel, and 0.02 error) - online methods. A job was designed for each alignment run and 16 threads were
reserved for each job. (https://www.med.upenn.edu/hpc/) -Intel E5-2665 2.4Ghz Xeon Processors (115 W)",,3:37:53,16,CPU,Used,12,UPENN HPC,1.58,Xeon E5-2665,,,3.63,14.4,0.234,1.422,,0.675,0.059,0.711,2.691,3.860,1.4,1.1,1.7,0.9,0.7,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.1,0.0
Jason,star_malaria,Reads alignments,star,,Benchmark,,,,,"The P. falciparum genome was used because it is notorious for being difficult, mainly because it is approximately 80% AT rich in exons and 90% in introns and intergenic regions - Simulated data from malaria containing 10 million 100 basre read pairs (2x10^9 bases) The most complex forms of the simulate dataset were used here (T3) which had high polymorphism and error rates (0.03 substitution, 0.005 indel, and 0.02 error) - online methods. A job was designed for each alignment run and 16 threads were
reserved for each job. (https://www.med.upenn.edu/hpc/) -Intel E5-2665 2.4Ghz Xeon Processors (115 W). from Simulation-based comprehensive benchmarking of RNA-seq aligners. ",,2:22:38,16,CPU,Used,8,UPENN HPC,1.58,Xeon E5-2665,,,2.38,14.4,0.233,0.925,,0.439,0.039,0.462,1.750,2.511,0.9,0.7,1.1,0.6,0.5,0.4,0.4,0.4,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Loïc,,Reads alignments,star,,Benchmark,,,,,"Reads alignment on simulated data for P. flaciparum genome (1 set of 10 million 100-base read pairs - 2x10^9 bases) from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5792058/). Used ""Total run time""",,2:27:19,16,CPU,,8,,,Xeon E5-2665,,,2.46,14.4,0.233,0.955,,0.454,0.040,0.478,1.808,2.593,0.9,0.7,1.1,0.6,0.5,0.5,0.4,0.4,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Loïc,,eQTL analysis,matrixeQTL,,Benchmark,,,,,"""We chose cystic fibrosis (CF) dataset from Wright et al.
(2011). The dataset contains genotype information for 573 337 SNPs
and the gene expression measurements for 22 011 transcripts for 840
patients.""",,0:14:36,4,CPU,,16,local server,1.58,Xeon X3430,,,0.24,23.8,0.101,0.041,,0.019,0.002,0.021,0.078,0.111,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jason/Loïc,,de novo virome assembly,Velvet,,Benchmark,,,,,"""Assemblers were compared for practicality by measuring the time to reach completion and maximum RAM usage via four published healthy human gut viromes (Manrique P, Bolduc B, Walk ST, van der Oost J, de Vos WM, Young MJ. Healthy human gut phageome. Proc Natl Acad Sci. 2016;113(37):10400–5.). Reads from the “healthy human gut phageome” were analysed to compare the overall assembler efficiency and the impact of sequencing depth. Reads were randomly subset in pairs (both the forward and reverse read of a pair were retained) to different depths using an in-house python script. Samples were subset in increments of 300,000 reads to their respective maximum depth (2.7, 3.5, 3 and 3.3 million reads) .It must be noted that all assembly tasks were allocated five threads. Each assembler with the exception of Geneious and CLC was run as per manual with default parameters (unless stated) using a Lenovo x3650 M5 server with an intel Xeon processor E5-2690 v3 and 512Gb RAM running Ubuntu 14.04.5. "" (from https://doi.org/10.1186/s40168-019-0626-5) --> Taking running time and RAM for 3.5m reads",,0:40:00,5,CPU,Used,0.01,,,Xeon E5-2690 v3,,,0.67,11.3,0.056,0.063,,0.030,0.003,0.031,0.119,0.170,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jason/Loïc,,de novo virome assembly,Spades,,Benchmark,,,,,same,,1:06:40,5,CPU,Used,0.008,,,Xeon E5-2690 v3,,,1.11,11.3,0.056,0.104,,0.050,0.004,0.052,0.198,0.283,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jason/Loïc,,de novo virome assembly,MEGAHIT,,Benchmark,,,,,Same,,0:43:20,5,CPU,Used,0.008,,,Xeon E5-2690 v3,,,0.72,11.3,0.056,0.068,,0.032,0.003,0.034,0.128,0.184,0.1,0.1,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Jason/Loïc,,metagenome assembly,meta-SPADES,,Benchmark,,,,,we here tested assemblers on real Illumina metagenome sequencing data from natural communities of varying complexity sampled from forest soil and algal biofilms. from (https://doi.org/10.1371/journal.pone.0169662) -< Measurements here are for the Marburg forest soil,,29:24:00,4,CPU,Used,60,,,Xeon E5-4650L,,,29.40,14.4,0.080,3.920,,1.862,0.163,1.960,7.419,10.641,3.8,2.9,4.6,2.4,1.9,1.9,1.5,1.8,1.3,1.3,1.1,1.5,0.5,0.2,0.4,0.1
,,Metagenoma assembly for 100 samples,meta-SPADES,,,,,,,,,.,.,.,,.,,,.,,,,,7.985,392.048,,186.223,16.335,196.024,741.923,"1,064.129",376.6,291.3,458.7,244.5,192.7,186.6,154.1,183.9,129.5,128.2,108.7,154.1,54.9,18.4,41.2,5.5
Jason/Loïc,,metagenome assembly,Megahit,,Benchmark,,,,,Same,,15:36:00,4,CPU,Used,12,,,Xeon E5-4650L,,,15.60,14.4,0.062,1.614,,0.767,0.067,0.807,3.055,4.382,1.6,1.2,1.9,1.0,0.8,0.8,0.6,0.8,0.5,0.5,0.4,0.6,0.2,0.1,0.2,0.0
,,Metagenoma assembly for 100 samples,Megahit,,,,,,,,,.,.,.,,.,,,.,,,,,6.197,161.444,,76.686,6.727,80.722,305.522,438.206,155.1,119.9,188.9,100.7,79.4,76.8,63.4,75.7,53.3,52.8,44.8,63.4,22.6,7.6,17.0,2.3
Jason/Loïc,,metagenome assembly,MetaVelvet k101,,Benchmark,,,,,Same,,1:06:00,8,CPU,Used,130,,,Xeon E5-4650L,,,1.10,14.4,0.163,0.300,,0.143,0.013,0.150,0.568,0.815,0.3,0.2,0.4,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0
Jason/Loïc,,metagenome assembly,MetaVelvet k21,,Benchmark,,,,,Same,,7:54:00,8,CPU,Used,267,,,Xeon E5-4650L,,,7.90,14.4,0.214,2.829,,1.344,0.118,1.415,5.354,7.680,2.7,2.1,3.3,1.8,1.4,1.3,1.1,1.3,0.9,0.9,0.8,1.1,0.4,0.1,0.3,0.0
,,same for 100 samples,MetaVelvet k21,,,,,,,,,.,.,.,,.,,,.,,,,,21.446,282.934,,134.394,11.789,141.467,535.432,767.963,271.8,210.2,331.0,176.4,139.1,134.6,111.2,132.7,93.4,92.5,78.5,111.2,39.6,13.3,29.7,4.0
Jason/Loïc,ABySS2.0_assembly,De novo human genome assembly,ABySS2.0,,Benchmark,,,,,"To assess the performance of ABySS 2.0, we compared it with other leading assemblers for large genomes: ABySS 1.0 (Simpson et al. 2009), BCALM 2 (Chikhi et al. 2016), DISCOVAR de novo, MEGAHIT (Li et al. 2016), Minia (Chikhi and Rizk 2013), SGA (Simpson and Durbin 2011), and SOAPdenovo2 (Luo et al. 2012).  We conducted our comparison using a recent, publicly available human short-read data set provided by the Genome in a Bottle (Zook et al. 2016) project. The NIST HG004 (Coriell cell line NA24143) data were chosen for its deep 70× coverage of Illumina short-read (paired-end 250 bp) data and the availability of sequences from other platforms, including a 175× physical coverage mate-pair data set (after trimming), 10x Genomics Chromium data, and BioNano optical mapping data. .All assemblies from Figure 3 were benchmarked on a server with 2.5 TB of RAM and four Xeon E7-8867 v3 CPUs running at 2.50 GHz, providing a total of 64 cores. from - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5411771/ -> Assembled GIAB HG004",,20:00:00,64,CPU,Used,34,,,Xeon E7-8867 v3,,,20.00,10.3,0.673,22.467,,10.672,0.936,11.234,42.517,60.982,21.6,16.7,26.3,14.0,11.0,10.7,8.8,10.5,7.4,7.3,6.2,8.8,3.1,1.1,2.4,0.3
Jason/Loïc,MEGAHIT_assembly,De novo human genome assembly,MEGAHIT,,Benchmark,,,,,Same,,26:00:00,64,CPU,Used,197,,,Xeon E7-8867 v3,,,26.00,10.3,0.733,31.843,,15.126,1.327,15.922,60.262,86.432,30.6,23.7,37.3,19.9,15.7,15.2,12.5,14.9,10.5,10.4,8.8,12.5,4.5,1.5,3.3,0.4
Jonathan,eQTL_LIMIX_1,LIMIX cis-eQTL calling Part I,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements. (3460 Chunks)",,9462:26:18,1,CPU,Requested,150,CSD3,1.15,Xeon Gold 6142,,,9462.44,9.4,0.065,1031.098,,489.772,42.962,515.549,"1,951.282","2,798.695",990.5,766.0,"1,206.4",643.0,506.9,490.7,405.2,483.6,340.5,337.2,285.9,405.2,144.4,48.5,108.4,14.4
Jonathan,eQTL_LIMIX_2,LIMIX cis-eQTL calling Part II,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements (65 Chunks)",,232:19:11,1,CPU,Requested,160,CSD3,1.15,Xeon Gold 6142,,,232.32,9.4,0.069,26.761,,12.711,1.115,13.380,50.642,72.636,25.7,19.9,31.3,16.7,13.2,12.7,10.5,12.6,8.8,8.8,7.4,10.5,3.7,1.3,2.8,0.4
Jonathan,eQTL_LIMIX_3,LIMIX cis-eQTL calling Part III,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements (1 Chunk)",,10:08:59,1,CPU,Requested,300,CSD3,1.15,Xeon Gold 6142,,,10.15,9.4,0.121,2.053,,0.975,0.086,1.027,3.885,5.573,2.0,1.5,2.4,1.3,1.0,1.0,0.8,1.0,0.7,0.7,0.6,0.8,0.3,0.1,0.2,0.0
Jonathan,,all together,,,,,,,,,,9704:54:28,1,CPU,,.,,,.,,,,,0.989,1091.755,,518.584,45.490,545.878,"2,066.071","2,963.336","1,048.7",811.1,"1,277.4",680.8,536.7,519.6,429.1,512.0,360.5,357.0,302.7,429.1,152.8,51.3,114.7,15.3
Jonathan,,Same with exact usage,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements. (3460 Chunks)",,9462:26:18,1,CPU,Used,40.71,,,Xeon Gold 6142,,,9462.44,9.4,0.025,387.779,,184.195,16.157,193.890,733.846,"1,052.544",372.5,288.1,453.7,241.8,190.6,184.5,152.4,181.9,128.0,126.8,107.5,152.4,54.3,18.2,40.8,5.4
Jonathan,,,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements (65 Chunks)",,232:19:11,1,CPU,Used,56.51,,,Xeon Gold 6142,,,232.32,9.4,0.030,11.804,,5.607,0.492,5.902,22.338,32.040,11.3,8.8,13.8,7.4,5.8,5.6,4.6,5.5,3.9,3.9,3.3,4.6,1.7,0.6,1.2,0.2
Jonathan,,,LIMIX HipSci Pipeline,,,,,,,"Pipeline developed by Marc Jan Bonder for fast eQTL mapping in the HIPSCI project. Splt into >3500 jobs to get around the 12h limit on CSD3, in three parts with different memory requirements (1 Chunk)",,10:08:59,1,CPU,Used,220.9,,,Xeon Gold 6142,,,10.15,9.4,0.092,1.554,,0.738,0.065,0.777,2.940,4.217,1.5,1.2,1.8,1.0,0.8,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.2,0.0
Jonathan,,eQTL all together,LIMIX HipSci Pipeline,,,,,,,,,9704:54:28,1,CPU,,.,,,.,,,,,0.147,401.137,,190.540,16.714,200.569,759.124,"1,088.801",385.3,298.0,469.3,250.1,197.2,190.9,157.6,188.1,132.5,131.2,111.2,157.6,56.2,18.9,42.2,5.6
Sergio,,MD simulation,AMBER,,not,,,,,"60.000 atoms, explicit solvent, 1ns simulation with periodic boundary conditions. ",,0:16:44,1,GPU,Requested,1,,,Tesla T4,,,0.28,70,0.070,0.033,,0.016,0.001,0.016,0.062,0.089,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Sergio,,MD simulation,AMBER,,not,,,,,"same, 3 replicates of 100ns",,83:40:00,.,.,,.,,,.,,,,,#VALUE!,#VALUE!,,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!,#VALUE!
Loïc,,MD simulation,AMBER18,,Benchmark,,,,,"MD simulation on Satellite Tobacco Mosaic Virus - STMV (1,066,628 atoms, periodic; 4fs timestep, 9A cutoff). Time for 100ns. Performances with a GPU V100: 32ns/day -> 2700 sec/ns -> 270k sec/100ns from https://ambermd.org/GPUPerformance.php
",,75:00:00,1,GPU,,0,,,Tesla V100,,,75.00,300,0.300,37.575,,17.848,1.566,18.788,71.108,101.989,36.1,27.9,44.0,23.4,18.5,17.9,14.8,17.6,12.4,12.3,10.4,14.8,5.3,1.8,3.9,0.5
Loïc,,MD simulation,NAMD 2.13,,Benchmark,,,,,"MD simulation on STMV (1,066,628 atoms, periodic; 2fs timestep with rigid bonds, 12A cutoff with PME every 2 steps). Time for 100ns. Performances with a GPU V100: 6ns/day -> 14,400 sec/ns -> 1,440,000 sec/100ns from https://www.ks.uiuc.edu/Research/namd/benchmarks/ (NAMD slower due to slightly different settings)",,400:00:00,1,GPU,,0,,,Tesla V100,,,400.00,300,0.300,200.400,,95.190,8.350,100.200,379.243,543.943,192.5,148.9,234.5,125.0,98.5,95.4,78.8,94.0,66.2,65.5,55.6,78.8,28.1,9.4,21.1,2.8
Loïc,,MD simulation,GROMAC,,,,,,,,,,,,,,,,,,,,,0.000,0.000,,0.000,0.000,0.000,0.000,0.000,,,,,,,,,,,,,,,,
Yu,,PGS level Calculation,MLP,,,,,,,"PGS model training using MLP for mean corpuscular volume (MCV) on UK biobank;
Model structure:'activation': 'relu', 'dropout': 0.0, 'nb_layers': 5, 'nb_neurons': 32, 'optimizer': 'adagrad', 'weight_decay': 0.0;
5 MLP models were trained;
Number of samples: 407157; Number of variants:739
(Note the model struture/hyperparameters were optimized beforehand, which repeated this unit for at least hundreds of times)",,0:09:14,1,GPU,Requested,24,,,Tesla P100 PCIe,,,0.15,250,0.259,0.067,,0.032,0.003,0.033,0.126,0.181,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
,,,,,,,,,,,,0:09:14,1,GPU,Used,8.3,,,Tesla P100 PCIe,,,0.15,250,0.253,0.065,,0.031,0.003,0.033,0.123,0.177,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Yu,,PGS level Calculation,CNN,,,,,,,"PGS model training using CNN for mean corpuscular volume (MCV) on UK biobank;
Model structure: 'activation': 'linear', 'batch_norm': True, 'dropout': 0.1, 'filters': 64, 'nb_cnn_layers': 1, 'nb_layers': 2, 'nb_neurons': 16, 'optimizer': 'rmsprop', 'size_window': 10, 'stride': 'one', 'weight_decay ': 0.0;
5 CNN models were trained;
Number of samples: 407157; Number of variants:739
(Note the model struture/hyperparameters were optimized beforehand,  which repeated this unit for at least hundreds of times)",,0:16:52,1,GPU,Requested,24,,,Tesla P100 PCIe,,,0.28,250,0.259,0.122,,0.058,0.005,0.061,0.230,0.330,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
,,,,,,,,,,,,0:16:52,1,GPU,Used,8.9,,,Tesla P100 PCIe,,,0.28,250,0.253,0.119,,0.056,0.005,0.059,0.225,0.323,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Yu,,PGS level Calculation,"ElasticNet and
Bayesian Ridge",,,,,,,"PGS model training using EN, BR for mean corpuscular volume (MCV) on UK biobank;
One EN model and one BR model were trained;
Number of samples: 407157; Number of variants:5000",,2:53:55,20,CPU,Requested,12.03,,,Xeon Gold 6142,,,2.90,9.4,0.192,0.929,,0.441,0.039,0.465,1.759,2.522,0.9,0.7,1.1,0.6,0.5,0.4,0.4,0.4,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
,,real usage,,,,,,,,,,2:53:55,20,CPU,Used,7.1,,,Xeon Gold 6142,,,2.90,9.4,0.190,0.920,,0.437,0.038,0.460,1.742,2.498,0.9,0.7,1.1,0.6,0.5,0.4,0.4,0.4,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Jason/Loïc,,Protein docking,AutoDock Vina,,benchmark,,,,,Average computing times (in seconds per ligand) on 4 DUD systems. Values for 1m ligands. from https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003571,,40972:13:20,1,CPU,,0.05,,,Xeon X5660,,,40972.22,15.8,0.016,1084.648,,515.208,45.194,542.324,"2,052.621","2,944.045","1,041.9",805.8,"1,269.0",676.4,533.2,516.2,426.3,508.7,358.2,354.7,300.8,426.3,151.9,51.0,114.0,15.2
Jason/Loïc,,Protein docking,Glide,,,,,,,,,1027:46:40,1,CPU,,0.05,,,Xeon X5660,,,1027.78,15.8,0.016,27.208,,12.924,1.134,13.604,51.489,73.851,26.1,20.2,31.8,17.0,13.4,12.9,10.7,12.8,9.0,8.9,7.5,10.7,3.8,1.3,2.9,0.4
Jason/Loïc,,Protein docking,rDock,,,,,,,,,12250:00:00,1,CPU,,0.05,,,Xeon X5660,,,12250.00,15.8,0.016,324.291,,154.038,13.512,162.146,613.699,880.220,311.5,240.9,379.4,202.2,159.4,154.3,127.4,152.1,107.1,106.0,89.9,127.4,45.4,15.2,34.1,4.5
Jason,,GWAS ,SAIGE v 0.29,,Benchmark,,,,,"The actual computation time and memory usage of association tests for the full UK Biobank data for CAD (Benchmarks for UK Biobank data coronary artery disease (PheCode 411)) are given in Table 1. 71 million variants that have an imputation info ≥ 0.3 for 408,458 samples. https://www.ncbi.nlm.nih.gov/pubmed/30104761",,517:22:48,1,CPU,Used,10.32,,,Ryzen 7 3800X,,,517.38,13.1,0.017,14.662,,6.964,0.611,7.331,27.746,39.796,14.1,10.9,17.2,9.1,7.2,7.0,5.8,6.9,4.8,4.8,4.1,5.8,2.1,0.7,1.5,0.2
Jason ,,GWAS ,BOLT-LMM v2.3.2,,Benchmark,,,,,,,360:37:48,1,CPU,Used,10.98,,,Ryzen 7 3800X,,,360.63,13.1,0.017,10.368,,4.925,0.432,5.184,19.620,28.141,10.0,7.7,12.1,6.5,5.1,4.9,4.1,4.9,3.4,3.4,2.9,4.1,1.5,0.5,1.1,0.1
Jason,,GWAS ,BOLT-LMM_INF_MODE v2.3.2 ,,,,,,,,,360:00:00,1,CPU,Used,10.98,,,Ryzen 7 3800X,,,360.00,13.1,0.017,10.350,,4.916,0.431,5.175,19.586,28.092,9.9,7.7,12.1,6.5,5.1,4.9,4.1,4.9,3.4,3.4,2.9,4.1,1.4,0.5,1.1,0.1
Loic,,Astrophysics1,,,,,,,,,,0:54:20,24,CPU,,80,,,Xeon E5-4620,,,0.91,11.9,0.315,0.476,,0.226,0.020,0.238,0.901,1.292,0.5,0.4,0.6,0.3,0.2,0.2,0.2,0.2,0.2,0.2,0.1,0.2,0.1,0.0,0.1,0.0
Loic,,NLP,BERT,,Strubell,,,,,,,79:00:00,64,GPU,,0,,,Tesla V100,,,79.00,300,19.200,2533.056,,"1,203.202",105.544,"1,266.528","4,793.632","6,875.438","2,433.3","1,881.8","2,963.7","1,579.6","1,245.3","1,205.5",995.5,"1,188.0",836.4,828.3,702.4,995.5,354.6,119.1,266.2,35.5
,,,,,,,,,,,"1,929.00",0:32:09,,,,,,,,,,0.54,#N/A,#N/A,#N/A,,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
,,,,,,,,,,,"30,864.00",8:34:24,,,,,,,,,,8.57,#N/A,#N/A,#N/A,,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Jason,geant4_1e7_1phi,Particle physics simulations,GEANT4,,,1e7 particles,Phi CPU,1,1 (60 cores),"[For 1e7 particles, using 60 Phi cores (1 Phi CPU)] Simulation of the passage of particles through matter using Geant4. Computing time in minutes of TestEm12 Geant4 example for 10^7 generated source particles. Estimated from figure 4, which Ignores hyperthreading just using the available cores  (10 Xeon and 60 Phi cores). No mention of memory used so i estimate with the memory from the following statements (The simulations have been performed on a machine having two Intel Xeon CPUs E5-2690v2 each capable of a theoretical 240.1 GFLOPS of double precision floating point instructions with 59.7GB/sec memory bandwidth, 128 GB of memory, and four Xeon Phi hardware accelerators 5110P each capable of theoretical 1010.5 GFLOPS of double precision floating point instructions with 320 GB/sec memory band- width and 8 GB of memory). so, I take the minimum of the two memories available, 2x 128GB (256GB in total) available for the Xeon, and 4 x 8GB (32 GB) in total available for the Phi. Therefore memory estimated to be 32GB as it is not directly mentioned in the paper and memory usage should be similar between the two CPUs. from-  http://dx.doi.org/10.1155/2015/980752, for detials about testem12 see https://iopscience.iop.org/article/10.1088/1742-6596/219/3/032044/pdf",,3:35:00,60,CPU,Available? ,32,,,Xeon Phi 5110P,,,3.58,3.8,0.237,1.418,,0.673,0.059,0.709,2.683,3.848,1.4,1.1,1.7,0.9,0.7,0.7,0.6,0.7,0.5,0.5,0.4,0.6,0.2,0.1,0.1,0.0
Jason,,Particle physics simulations,GEANT4 10.0 xeon,,,,,,,,,7:44:00,10,CPU,Available? ,32,,,Xeon E5-2690 v2,,,7.73,13.0,0.142,1.833,,0.871,0.076,0.916,3.469,4.975,1.8,1.4,2.1,1.1,0.9,0.9,0.7,0.9,0.6,0.6,0.5,0.7,0.3,0.1,0.2,0.0
Loïc,geant4_1e7_4phi,Particle physics simulations,GEANT4,,,1e7 particles,Phi CPU,4,4 (240 cores),"For 1e7 particles, using 240 Phi cores (4 Phi CPUs)",,1:40:00,240,CPU,,32,,,Xeon Phi 5110P,,,1.67,3.8,0.912,2.538,,1.206,0.106,1.269,4.803,6.889,2.4,1.9,3.0,1.6,1.2,1.2,1.0,1.2,0.8,0.8,0.7,1.0,0.4,0.1,0.3,0.0
Loïc,geant4_1e8_4phi,Particle physics simulations,GEANT4,,,1e8 particles,Phi CPU,4,4 (240 cores),"For 1e8 particles, using 240 Phi cores (4 Phi CPUs)",,16:40:00,240,CPU,,32,,,Xeon Phi 5110P,,,16.67,3.8,0.912,25.382,,12.056,1.058,12.691,48.033,68.893,24.4,18.9,29.7,15.8,12.5,12.1,10.0,11.9,8.4,8.3,7.0,10.0,3.6,1.2,2.7,0.4
Loïc,geant4_1e8_4phi_PSF,Particle physics simulations,GEANT4,,,1e8 particles,Phi CPU,4,4 (240 cores),Same but with PSF (=2),2,16:40:00,240,CPU,,32,,,Xeon Phi 5110P,,,16.67,3.8,0.912,50.764,,24.113,2.115,25.382,96.066,137.787,48.8,37.7,59.4,31.7,25.0,24.2,20.0,23.8,16.8,16.6,14.1,20.0,7.1,2.4,5.3,0.7
Loïc,geant4_1e8_2xeon,Particle physics simulations,GEANT4,,,1e8 particles,Xeon E5,2,2 (20 cores),"For 1e8 particles, using 20 Xeon E5 cores (2 Intel Xeon E5 CPUs)",,52:42:00,20,CPU,,32,,,Xeon E5-2690 v2,,,52.70,13.0,0.272,23.931,,11.367,0.997,11.966,45.289,64.957,23.0,17.8,28.0,14.9,11.8,11.4,9.4,11.2,7.9,7.8,6.6,9.4,3.4,1.1,2.5,0.3
Loïc,geant4_em_1e7_1_xeon,Electro-magnetic simulations,GEANT4,Methods,,Xeon_vs_Phi,,,,"[Xeon] Comparison between Xeon and Phi (figure 4 of the article) on 1e7 particles, 1 Xeon vs 1 phi",,7:40:00,10,CPU,,128,,,Xeon E5-2690 v2,,,7.67,13.0,0.178,2.275,,1.081,0.095,1.137,4.305,6.175,2.2,1.7,2.7,1.4,1.1,1.1,0.9,1.1,0.8,0.7,0.6,0.9,0.3,0.1,0.2,0.0
Loïc,geant4_em_1e7_1_phi,Electro-magnetic simulations,GEANT4,Methods,,Xeon_vs_Phi,,,,[phi] same comparison as above,,3:30:00,60,CPU,,8,,,Xeon Phi 5110P,,,3.50,3.8,0.228,1.333,,0.633,0.056,0.666,2.522,3.617,1.3,1.0,1.6,0.8,0.7,0.6,0.5,0.6,0.4,0.4,0.4,0.5,0.2,0.1,0.1,0.0
Loïc,geant4_em_1e8_1core,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,1,,,[1 thread],,300:00:00,1,CPU,,8,,,Xeon Phi 5110P,,,300.00,3.8,0.007,3.372,,1.602,0.140,1.686,6.381,9.152,3.2,2.5,3.9,2.1,1.7,1.6,1.3,1.6,1.1,1.1,0.9,1.3,0.5,0.2,0.4,0.0
Loïc,geant4_em_1e8_3cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,3,,,"[10 thread] 1e8 particles, comparing the impact of the number of threads (figure 7 and table 2 in the paper)",,30:00:00,3,CPU,,8,,,Xeon Phi 5110P,,,30.00,3.8,0.014,0.713,,0.339,0.030,0.356,1.349,1.935,0.7,0.5,0.8,0.4,0.4,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.1,0.0,0.1,0.0
Loïc,geant4_em_1e8_5cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,5,,,[20 threads],,15:01:48,5,CPU,,8,,,Xeon Phi 5110P,,,15.03,3.8,0.022,0.545,,0.259,0.023,0.273,1.032,1.480,0.5,0.4,0.6,0.3,0.3,0.3,0.2,0.3,0.2,0.2,0.2,0.2,0.1,0.0,0.1,0.0
Loïc,geant4_em_1e8_10cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,10,,,[40 threads],,7:31:55,10,CPU,,8,,,Xeon Phi 5110P,,,7.53,3.8,0.040,0.509,,0.242,0.021,0.255,0.964,1.382,0.5,0.4,0.6,0.3,0.3,0.2,0.2,0.2,0.2,0.2,0.1,0.2,0.1,0.0,0.1,0.0
Loïc,geant4_em_1e8_15cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,15,,,[60 threads],,4:59:27,15,CPU,,8,,,Xeon Phi 5110P,,,4.99,3.8,0.059,0.494,,0.234,0.021,0.247,0.934,1.340,0.5,0.4,0.6,0.3,0.2,0.2,0.2,0.2,0.2,0.2,0.1,0.2,0.1,0.0,0.1,0.0
Loïc,geant4_em_1e8_30cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,30,,,[120 threads],,3:11:27,30,CPU,,8,,,Xeon Phi 5110P,,,3.19,3.8,0.115,0.615,,0.292,0.026,0.308,1.165,1.670,0.6,0.5,0.7,0.4,0.3,0.3,0.2,0.3,0.2,0.2,0.2,0.2,0.1,0.0,0.1,0.0
Loïc,geant4_em_1e8_60cores,Electro-magnetic simulations,GEANT4,Methods,,threads_comparison,60,,,[240 threads],,2:37:37,60,CPU,,8,,,Xeon Phi 5110P,,,2.63,3.8,0.228,1.000,,0.475,0.042,0.500,1.893,2.715,1.0,0.7,1.2,0.6,0.5,0.5,0.4,0.5,0.3,0.3,0.3,0.4,0.1,0.0,0.1,0.0
Loïc,geant4_DNA_1energy,Irradiation simulation,GEANT4-DNA,Methods,,,,,,"[1 run = 1 level of energy] Evaluation of the dammages to DNA due to radiations, using the GEANT4-DNA extension. It's not a real benchmark and we don't have very precise values, but it's a fairly good estimate I think. They look at the impact on the entire human genome (6.4e9 nucleotide pairs). 5,000 particles simulated for each energy level, 11 energy levels, split across 11 nodes. 24 threads per node, including multi-therading, so we can assume it's maybe 12 physical cores (and 24 logical cores). Computations lasted ~3 weeks. The choice of the CPU is unknown, so we're picking a standard value (with 12 cores to match the 24 cores setup). From [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5607336/] ",,504:00:00,12,CPU,,10,,,Xeon E5-2680 v3,,,504.00,10.0,0.124,104.137,,49.465,4.339,52.068,197.072,282.657,100.0,77.4,121.8,64.9,51.2,49.6,40.9,48.8,34.4,34.1,28.9,40.9,14.6,4.9,10.9,1.5
Loïc,geant4_DNA_11energies,Irradiation simulation,GEANT4-DNA,Methods,,,,,,Same but with PSF=11 for the different energy levels,11,504:00:00,12,CPU,,10,,,Xeon E5-2680 v3,,,504.00,10.0,0.124,1145.505,,544.115,47.729,572.753,"2,167.789","3,109.229","1,100.4",851.0,"1,340.2",714.3,563.1,545.1,450.2,537.2,378.2,374.6,317.6,450.2,160.4,53.8,120.4,16.0
Jason,,"kilometre-scale global weather and climate simulations, considering the Icosahedral Non-hydrostatic (ICON) model",Icosahedral Non-hydrostatic (ICON) model,,Benchmark,,,,,"Global resolution of 5km for global weather and climate simulations of ICON. ICON uses a finite difference approximation of the non-hydrostatic equations of atmospheric motion, Earth is discretized via refinement of an initial globe- covering icosahedron. This refinement results in ca 21 million horizontal cells (triangles) to reach a global resolution of ca 5 km and a time step of 45 s. International project DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND), ICOsahedral Non-hydrostatic (ICON) model; 

Each dual-socket node of the partition features two Broadwell CPUs with 18 compute cores and a minimum of 64 GB main memory.

""Since the ICON-DYAMOND 5 km configuration needs to be executed on at least 100 nodes due to memory requirements"" - assumming that all 6400GB memory was needed

So the 100 node estimation would have 1800 cores and perhaps 6400 Gb memory. Note that ""hyperthreading was enabled and 6 OpenMP threads per MPI process were employed"" however, our calculations ignore hyperthreading.

estimated from figure 5 in (6400 + (12800-6400)*0.25) http://dx.doi.org/10.1098/rsta.2018.0148",,2:13:20,1800,CPU,Available? ,6400,,,Xeon E5-2695 v4,,,2.22,6.7,14.384,53.381,,25.356,2.224,26.690,101.019,144.890,51.3,39.7,62.5,33.3,26.2,25.4,21.0,25.0,17.6,17.5,14.8,21.0,7.5,2.5,5.6,0.7
Loïc,weather_ICON_1FD,Weather forecast,ICON,Methods,Benchmark,1FD,,,,"[1 FD]  ICON is run by the german DWD to make predictions at 13km and takes 8min per forecast day (FD). That's 365/8=180 FD/day (in line with the 200-300 FD/day given by Neumann et al.). Based on Neumann et al., for a 10km simulation, ICON achieves 78 FD/day with 250 broadwell cores (36 cores each, 64GB memory per node). So to achieve 180 FD/day, it needs 575 nodes. The location of the DMRZ (HPC of DWD) is Germany (we use the hardware frorm the benchmark, but the location form the real application....which happen to be the same CPUs, they are all equiped with the Cray XC40 setup). Benchmark from [https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0148] and specs of the HPC used for the benchmark [https://www.dkrz.de/systems/hpc/hlre-3-mistral]",,0:08:00,20700,CPU,requested,36800,,,Xeon E5-2695 v4,,,0.13,6.7,151.708,33.780,Germany,15.843,1.390,16.677,63.119,90.531,32.4,25.1,39.5,21.1,16.6,16.1,13.3,15.8,11.2,11.0,9.4,13.3,4.7,1.6,3.6,0.5
Loïc,weather_ICON_180FD,Weather forecast,ICON,Methods,Benchmark,180FD,,,,"Same, scaled to 180 FD (PSF = 180)",180,.,.,.,.,.,,,.,.,,24.00,1200.00,27307.44,6080.46,Germany,"2,851.734",250.152,"3,001.825","11,361.491","16,295.624","5,840.9","4,517.2","7,114.1","3,791.8","2,989.2","2,893.7","2,389.6","2,851.7","2,007.8","1,988.3","1,686.1","2,389.6",851.3,285.8,639.1,85.1
Loïc,weather_IFS_1FD_reading,Weather forecast,IFS,Methods,Benchmark,1FD,"Reading, UK",,,"Operational weather forecast at ECMWF: TCo1279 9km resolution, 137 vertical levels for a 10-day deterministic forecast, double precision. Based on figure 4, we can work out that it takes 128 nodes to achieve 180 forecast days per day (8min/FD). That's 128*36=4,608 cores and 128*64=8,192 GB of memory. Hardware described here: [https://www.ecmwf.int/en/computing/our-facilities/supercomputer] / Double precision: [https://www.ecmwf.int/en/newsletter/157/meteorology/progress-using-single-precision-ifs]",,0:08:00,4608,CPU,requested,8192,,,Xeon E5-2695 v4,1.45,https://www.ecmwf.int/sites/default/files/elibrary/2011/17450-green-computing.pdf,0.13,6.7,33.772,6.529,"Reading, UK (https://www.zdnet.com/article/europes-big-weather-supercomputer-data-center-is-about-to-leave-uk/)",1.811,0.159,1.906,7.213,10.346,6.3,4.9,7.6,4.1,3.2,3.1,2.6,3.1,2.2,2.1,1.8,2.6,0.9,0.3,0.7,0.1
,weather_IFS_180FD_reading,Weather forecast,IFS,Methods,Benchmark,180FD,"Reading, UK",,,"Same, scaled to 180 FD (PSF = 180)",180,.,.,.,.,.,,,.,.,.,24.00,1200.00,6078.87,1175.25,"Reading, UK",325.897,28.587,343.049,"1,298.393","1,862.266","1,128.9",873.1,"1,375.0",732.9,577.8,559.3,461.9,551.2,388.1,384.3,325.9,461.9,164.5,55.2,123.5,16.5
Loïc,weather_IFS_1FD_bologna,Weather forecast,IFS,Methods,Benchmark,1FD,"Bologna, IT",,,"Same but in Bologna, for 1 FD",,0:08:00,4608,CPU,requested,8192,,,Xeon E5-2695 v4,1.27,https://www.ecmwf.int/sites/default/files/medialibrary/2019-02/Industry_day_-_QA.pdf,0.13,6.7,33.772,5.719,"Bologna, IT",1.870,0.164,1.968,7.450,10.686,5.5,4.2,6.7,3.6,2.8,2.7,2.2,2.7,1.9,1.9,1.6,2.2,0.8,0.3,0.6,0.1
,weather_IFS_180FD_bologna,Weather forecast,IFS,Methods,Benchmark,180FD,"Bologna, IT",,,"Same, scaled to 180 FD (PSF = 180)",180,.,.,.,.,.,,,.,.,.,24.00,1200.00,6078.87,1029.36,"Bologna, IT",336.599,29.526,354.315,"1,341.033","1,923.425",988.8,764.7,"1,204.3",641.9,506.0,489.9,404.5,482.8,339.9,336.6,285.4,404.5,144.1,48.4,108.2,14.4
Jason,,Plane-Wave Self Consistent Field calculations for surface of 112 gold atoms and 2 k points (AUSURF112),Quantum Espresso - PWscf,,Benchmark,,,,,"Plane-Wave Self Consistent Field calculations for a gold surface with 112 gold atoms and 2 k points as well as a calculations for tantalum pentoxied with 96 atoms and 26 kpoints. See table 1 for input specifications about these different cases. 

The basic computations of the PWscf code involve the calculation of the Kohn-Sham (KS) orbitals (that arise in density functional theory) and energies for isolated or extended/periodic systems and the complete structural optimizations of the microscopic (atomic coordinates) and macroscopic (unit cell) degrees of freedom. The KS orbitals are quantum-mechanical states of electrons under an effective Kohn-Sham potential. 

Estimated using the Broadwell CPU system, cannot estimate the GPU systems as they both utilise CPUs and GPUs simoultaneously - ""The reference CPU system (labeled “Broadwell” in the results) is a private development system of a few hundred nodes fully based on Intel technology. Each node has dual socket 18-core Intel Xeon E5-2697 v4 (Broadwell) CPUs, 128 GB of system memory"".  

AUSURF112 can be run on two CPUs (18*2 cores), so assumming that 128GB * 2 (as two CPUs are requested as smallest case) = 256GB memory minimum, 26 CPUs needed for TA205 26k point calc, so 18*26 cores and 26*128GB is 3328GB forTA2O5 case.

from - https://doi.org/10.1007/978-3-319-72971-8_4",,0:19:50,36,CPU,minimum avaliable?,256,,,Xeon E5-2697 v4,,,0.33,8.1,0.385,0.213,,0.101,0.009,0.106,0.403,0.577,0.2,0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.0,0.0,0.0,0.0
Jason,,Plane-Wave Self Consistent Field calculations for tantalum pentoxide with 96 atoms and 26 k points (Ta2O5),,,,,,,,,,0:50:55,468,CPU,minimum avaliable?,3328,,,Xeon E5-2697 v4,,,0.85,8.1,5.010,7.101,,3.373,0.296,3.550,13.438,19.273,6.8,5.3,8.3,4.4,3.5,3.4,2.8,3.3,2.3,2.3,2.0,2.8,1.0,0.3,0.7,0.1
Jason,,NN genomic prediction,Keras,,Not Benchmark,,,,,"100455 SNPs used to train 14 NNs (hyperparameter optimisation) ,  and predict on test set of 40k",,72:00:00,1,GPU,Requested,32,,,Tesla T4,,,72.00,70,0.082,9.850,,4.679,0.410,4.925,18.641,26.736,9.5,7.3,11.5,6.1,4.8,4.7,3.9,4.6,3.3,3.2,2.7,3.9,1.4,0.5,1.0,0.1
Jason,,NN genomic prediction,Keras,,Not Benchmark,,,,,"100455 SNPs used to train 14 NNs (hyperparameter optimisation) ,  and predict on test set of 40k",,180:00:00,2,CPU,Requested,32,,,Xeon E5-2683 v4,,,180.00,7.5,0.027,8.092,,3.844,0.337,4.046,15.314,21.964,7.8,6.0,9.5,5.0,4.0,3.9,3.2,3.8,2.7,2.6,2.2,3.2,1.1,0.4,0.9,0.1
Loïc,BERT_us_StrubellPUE,NLP,BERT,Methods,Benchmark,,,,,Using the same settings as Strubell et al. but assuming 100% use of CPU to compare with their result. This assumes the improved performances from https://web.archive.org/web/20190521104957/https://medium.com/future-vision/bert-meets-gpus-403d3fbed848,,79:00:00,64,GPU,,0,,,Tesla V100,1.58,,79.00,300,19.200,2396.544,,"1,138.358",99.856,"1,198.272","4,535.292","6,504.905","2,302.1","1,780.4","2,804.0","1,494.5","1,178.1","1,140.5",941.8,"1,124.0",791.3,783.7,664.6,941.8,335.5,112.6,251.9,33.6
Loïc,BERT_us,NLP,BERT,Methods,,,,,,Same but using the updated PUE,,79:00:00,64,GPU,,0,,,Tesla V100,,,79.00,300,19.200,2533.056,,"1,203.202",105.544,"1,266.528","4,793.632","6,875.438","2,433.3","1,881.8","2,963.7","1,579.6","1,245.3","1,205.5",995.5,"1,188.0",836.4,828.3,702.4,995.5,354.6,119.1,266.2,35.5
Loïc,BERT_us_100,NLP,BERT,Methods,,,,,,Same but including PSF,100,79:00:00,64,GPU,,0,,,Tesla V100,,,79.00,300,19.200,253305.600,,"120,320.160","10,554.400","126,652.800","479,363.187","687,543.771","243,325.4","188,180.7","296,367.6","157,961.4","124,525.0","120,548.1","99,549.1","118,800.3","83,641.5","82,830.9","70,241.6","99,549.1","35,462.8","11,905.4","26,622.4","3,546.3"
Loïc,transformer_small_us_StrubellPUE,NLP,Transformer (small),Methods,Benchmark,,,,,"same, trnasformer (base)",,12:00:00,8,GPU,,0,,,Tesla P100 PCIe,1.58,,12.00,250,2.000,37.920,,18.012,1.580,18.960,71.761,102.926,36.4,28.2,44.4,23.6,18.6,18.0,14.9,17.8,12.5,12.4,10.5,14.9,5.3,1.8,4.0,0.5
Loïc,transformer_big_us_StrubellPUE,NLP,Transformer (big),Methods,,,,,,"same, trnasformer (big)",,84:00:00,8,GPU,,0,,,Tesla P100 PCIe,1.58,,84.00,250,2.000,265.440,,126.084,11.060,132.720,502.327,720.480,255.0,197.2,310.6,165.5,130.5,126.3,104.3,124.5,87.6,86.8,73.6,104.3,37.2,12.5,27.9,3.7
Loïc,ELMo_us_StrubellPUE,NLP,ELMo,Methods,,,,,,,,336:00:00,3,GPU,,0,,,Tesla P100 PCIe,1.58,,336.00,250,0.750,398.160,,189.126,16.590,199.080,753.490,"1,080.720",382.5,295.8,465.8,248.3,195.7,189.5,156.5,186.7,131.5,130.2,110.4,156.5,55.7,18.7,41.8,5.6
Loïc,NAS_us_StrubellPUE,NLP,NAS,Methods,,,,,,,,274120:00:00,8,GPU,,0,,,Tesla P100 PCIe,1.58,,274120.00,250,2.000,866219.200,,"411,454.120","36,092.467","433,109.600","1,639,259.442","2,351,166.400","832,090.2","643,514.2","1,013,476.5","540,174.3","425,833.4","412,233.7","340,424.1","406,256.8","286,025.6","283,253.7","240,202.6","340,424.1","121,270.7","40,712.3","91,039.6","12,127.1"
Loïc,,Astrophysics,Helios-r2,,Benchmark,,,,,from https://iopscience.iop.org/article/10.3847/1538-4357/ab6d71/pdf,,0:04:25,1,GPU,,0,,,RTX 2080,,,0.07,215,0.215,0.026,,0.013,0.001,0.013,0.050,0.072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
,,,,,,,,,,,,,,,,,,,,,,0.00,#N/A,#N/A,#N/A,,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
,,,,,,,,,,,,,,,,,,,,,,0.00,#N/A,#N/A,#N/A,,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
,,,,,,,,,,,,,,,,,,,,,,0.00,#N/A,#N/A,#N/A,,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A